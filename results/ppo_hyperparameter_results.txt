PPO Hyperparameter Tuning Results
==================================================
Run 1: Avg Reward = -1045.00
Parameters: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'clip_range': 0.2}

Run 2: Avg Reward = -200.00
Parameters: {'learning_rate': 0.0001, 'n_steps': 1024, 'batch_size': 32, 'clip_range': 0.1}

Run 3: Avg Reward = -200.00
Parameters: {'learning_rate': 0.0005, 'n_steps': 4096, 'batch_size': 128, 'clip_range': 0.3}

Run 4: Avg Reward = -200.00
Parameters: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 20}

Run 5: Avg Reward = -675.00
Parameters: {'learning_rate': 0.0002, 'n_steps': 2048, 'batch_size': 64, 'gae_lambda': 0.9}

Run 6: Avg Reward = -200.00
Parameters: {'learning_rate': 0.0003, 'n_steps': 1024, 'batch_size': 64, 'ent_coef': 0.01}

Run 7: Avg Reward = -235.00
Parameters: {'learning_rate': 0.001, 'n_steps': 2048, 'batch_size': 32, 'vf_coef': 1.0}

Run 8: Avg Reward = -930.00
Parameters: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'max_grad_norm': 1.0}

Run 9: Avg Reward = -200.00
Parameters: {'learning_rate': 0.0003, 'n_steps': 512, 'batch_size': 64, 'gamma': 0.95}

Run 10: Avg Reward = -700.00
Parameters: {'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 256, 'clip_range': 0.15}

